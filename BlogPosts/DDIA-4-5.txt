title:Chapter 4: Encoding and Evolution
 

Chapter 1 introduced the concept of “evolvability”: we should aim to build systems that make it easy to adapt to change. In most cases, this involves changes to the data that a system stores. 

 

<b>Adapting to changes</b>:

Relational databases typically enforce a schema that can be changed through migrations. Only one schema is enforced at any time. Schema-on-read (schemaless) databases don’t enforce a schema so there is a mixture of old and new data within the database.

When you make changes, you may not want to roll out everything all at once as this creates more risk. Likewise, when updates are on user devices, you can’t typically control when they will be updated. All of this means that you will have old and new versions of the software running at the same time, and therefore old and new data coming into and being read out of you application. 

To ensure your application continues to run smoothly, you need:

<ul><li><b>Backward compatibility</b>: Newer code can read data that was written by older code.</li><li><b>Forward compatibility</b>: Older code can read data that was written by newer code.</li></ul>

Backward compatibility is typically not hard to achieve as you have the benefit of knowing what that older code looks like when you write the newer code. Forward compatibility is a little but trickier and often involves your application code ignoring additions or changes made by newer versions of the code.

 

<b>Formats for Encoding Data</b>:

Programs typically work with two representation of data:

<ul><li><b>In memory</b>: objects, structs, lists, arrays, etc. that are typically accessed via pointers.</li><li><b>Sequence-of-byte representations</b>: when you write data to a file or send it over a network, you have to encode it to some sort of self-contained sequence of bytes. Since a pointer would not make sense to another process (as they do not share memory), this looks quite different from the data structures you typically use. </li></ul>

To work with these two different representations, we need a translation process. Translating from in-memory to a byte sequence is called encoding (also called: serialization, marshalling) and the reverse is called decoding (parsing, deserialization, unmarshalling).

 

<b>Language Specific Encodings</b>:

Most languages come with built in encoding. Unfortunately, these built in encoders come with several problems: you are now tightly coupled to one programming language (and if you want backwards computability, then you are tightly coupled forever!), they can come with security concerns, and many of them are notoriously slow.

 

<b>JSON, XML, Binary Variants</b>:

Standardized encodings can be used by many programming languages. JSON, XML, and CSV have the benefit of being human readable. All 3 have some issues around encoding numbers (the difference between a number and a string that happens to contain all numbers is ambiguous in XML and CSV; issues with large numbers), lack of support for binary strings, and schema issues (optional in XML and JSON, not supported in CSV). Despite these flaws, these are widely used, standardized formats and the flaws are greatly outweighed by the difficulty it takes to get different organizations to agree on a standard.

For internal use, you don’t need the overhead or baggage that comes with standardized encodings. Instead, it can be convenient to use binary encodings of your data. Options for this include:

<ul><li>Schemaless: your binary encoding with have to include key-value pairs, but won’t be tied to a schema.</li><li>Schema: your binary encodings will be more compact. Rather then including keys, you can include a schema version and perhaps afield tag (as used in Thrift’s BinaryProtocol). Avro is even more compact and includes no filed tags, instead requiring values to be in the same order as they appear in the schema.</li></ul>

Enforcing schemas also has the benefit of being a form of documentation – one that you are required to update, unlike the stuff that ends up in places like your ReadMe.

 

<b>Dataflows</b>:

<u>Databases</u>: 

Writes encode the data and reads decode it. You can think of writes as sending a message to your future self – and your future self better believe in backwards compatibility if they want to be able to read that message in the future. As mentioned earlier, you will likely have older and newer versions of your code running at once. You will need backwards and forwards compatibility to be able to function smoothly. There are additional issues: A newer version writes some data that includes a new field (we will call it X), an older version then accesses this data entry and updates it. The desired outcome would be for X to not get overwritten, even though the older version of the code doesn’t even know that an X field exists. This is often handled at the database level but it can be good to be aware of this (and consider handling it) at the application code level as well.

Code will often change, while there could be data in your database that is years old: data outlives code.

 

<u>Services – REST and RPC</u>:

Data needs to be communicated over a network. The most common arrangement is to have two roles: <b>clients</b> and <b>servers</b>. Servers expose an API over the network and clients connect and make requests to that server. The API exposed by the server is known as a <b>service</b>. While databases allow for arbitrary queries based on a query language, services restrict consumers to only what is allowed via the API.

Servers can even be clients to other servers: in service-oriented architecture (SOA) (or microservices architecture), you decompose your application into smaller services. This might include servers that call other servers during the lifecycle of a request.  A benefit or microservices is to make different parts of the application independently deployable, allowing for rapid change. Typically, one team would own its own microservice.

When HTTP is used as the underlying protocol for talking to a service, it is called a web service (though they are not only used on the web). There are two popular approaches to web services: <b>REST</b> and <b>SOAP</b>. REST is not a protocol but rather a design philosophy built on HTTP: simple data formats, URLs for identifying resources, authentication, content type negotiation. An API designed according to REST principles is RESTful. 

SOAP is XML-based protocol for web requests and aims to be independent of HTTP. The API of a SOAP web service is described in an XML-based language called the Web Services Description Language (WSDL). WSDL is not designed to be human readable. 

REST is more popular for cross-organization communication and microservices. Although it is still used at many large enterprises, it has fallen out of favor at many smaller companies.

 

 

Remote Procedure Call (RPC) has been around since the 1970s. RPC tries to make a request to a remote network service look the same as calling a function in your programming language. Although convenient at first, this is fundamentally flawed as making a network request is very different from making a local function call: 

<ul><li>A local function call is predictable (fails or succeeds), a network call is unpredictable: the request or response may be lost or delayed; the remote machine may be slow or unavailable.</li><li>Retrying a failed network call may actually be the same as rerunning it if the first request succeeded but the response was lost.</li><li>Latency is widely variable on a network call while local calls are typically similar in runtime.</li><li>You need to encode parameters for a network call, while a local call can accept pointers.</li></ul>

REST does not try to hide the fact that it is a network protocol. REST is the predominant style for public APIs. RPC frameworks are often used between services owned by the same organization.

gRPC is a RPC implementation using Protocol Buffers.

 

<u>Message-Passing</u>:

REST and RPC: 

One process sends a request and another tries to respond as quickly as possible. 

Databases: 

One process writes data and another process might read it sometime in the future. 

Asynchronous message passing systems are somewhere in between RPC and databases. A message is delivered to a client, typically with low latency (similar to RPC). The message does not go directly to the server, but rather to a message broker (also called: message queue, or message-oriented middleware) (similar to a database). 

Benefits of a message broker: 

<ul><li>Can help with load if messages are arriving too fast</li><li>Can redeliver messages if server has crashed.</li></ul> 

Typically a message is sent to a queue or topic, and those messages are delivered to all subscribers of that topic. There can be multiple producers and consumers of the same topic. The dataflow is one way and there is typically no enforced schema.

 
line:



Title:Part II: Distributed Data
 

Part II takes us from data systems that store data on a single machine, to multiple machines that store and retrieve data. There are various reasons for doing this:

<ul><li><b>Scalability</b>: Having data on multiple machines allows you to spread the load between machines.</li><li><ul><li><b>Scaling up</b>: The simplest approach to scalability is to scale up (aka vertical scaling): buy a more powerful machine. Many CPUs, RAM chips and discs can be joined together under a single operating system and this allows any CPU to access any part of memory: shared-memory architecture – all components are treated as a single machine. The problem is that a machine twice as powerful usually costs more than twice as much. Scaling up offers some fault tolerance (hot swappable components) but data is still tied to one geographical location.</li><li><b>Shared-disk architecture</b>: Several machines with independent CPUs and RAM but a shared memory over a fast network. Typically used for data warehousing workloads.</li><li><b>Scaling out: Shared-nothing architecture</b> (horizontal scaling or scaling out): independent machines or virtual machines called <b>nodes</b>. Coordination between nodes is done on the software level over a traditional network. No special hardware required, you can use cheaper machines and with cloud deployments of virtual machines, you don’t even need to be a huge company to take advantage of multi-region distributed architecture.</li></li></ul><li><b>Fault tolerance/high availability</b>: Multiple machines provides redundancy and allows you to keep running even if single nodes go down.</li><li><b>Latency<b>: Servers around the world can be closer to users and therefore speed up response times.</li></ul>

 

Two common ways to distribute data across multiple nodes:

<ul><li><b>Replication</b>: The same data copied on different nodes. Provides redundancy. Can also improve performance as load can be spread out and data can be geographically closer to users.</li><li><b>Partitioning</b>: Splitting a large database into smaller subsets called partitions (also called sharding).</li></ul>

You can both replicate and shard your data.

line:

title:Chapter 5: Replication
 

<quote>“The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repair.”</quote>

- Douglas Adams, “Mostly Harmless” (1992)

 

Redundancy means keeping a copy of the same data on multiple machines that are connected via a network.

<ul><li>To keep data geographically close, thus reducing latency</li><li>To allow the system to work even if some parts go down, thus increasing availability</li><li>To scale out the number of machines serving read queries, thus increasing read throughput</li></ul>

 

All of the difficulty in replication lies in handling changes to replicated data. Popular algorithms include:

<ul><li>Single-leader</li><li>Multi-leader</li><li>Leaderless replication.</li></ul>

 
 
  

<b>Single Leader Replication</b>:

Each node that stores a copy of the data is called a <b>replica</b>. With each write, we need to ensure that each replica receives those changes to the data. The most common solution is called <b>leader-based replication</b> (aka active/passive, master-slave replication). Here’s how it works:

<ol><li>One replica is designated as the <b>leader</b> (master, primary). New writes are directed to the leader which first updates its local copy of the data.</li><li>The other replicas are known as followers (read replicas, slaves, secondaries, hot standbys). Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers via a <b>replication log</b> or change stream. Each follower takes those changes and applies them to their local storage in the same order as the leader.</li><li>When a client wants to read from the database, they can query the leader or any of its followers.</li></ol>

 

Follower updates can be <b>synchronous</b> or <b>asynchronous</b>. Synchronous means that when a write comes in the following happens: 

<ul><li>The leader updates its local storage and sends out a change stream</li><li>The followers must then all successfully update their local storage and send back a response before the leader will report to the client that the write was successful. </li></ul>

This has the advantage that all copies of the data are always up to date. The major disadvantage is that a single node going down can grind the whole system to a stop (a follower not being able to acknowledge an write prevents writes from succeeding). For this reason, many systems that are synchronous really just mean that at least one follower is synchronous. If that follower goes down, another replica is made synchronous. This is sometimes known as <b>semi-synchronous</b>. This ensures that if the leader goes down, at least one other replica has the most recent writes.

Often though, leader-based replication is completely asynchronous. If the leader fails and is not recoverable, and has not yet been replicated, this means that writes are lost even if they have been confirmed to the client. This means that a write is not guaranteed to be <b>durable</b>, even if it has been confirmed to the client. For example, a user performs an action that updates a page. The user expects that change to be durable (i.e. to persist). Due to an outage on the leader node, that update is lost and when the user checks again, their changes seem to have disappeared. The advantage of asynchronous replication is that the leader can continue to process writes even if many or all of the followers have fallen behind or gone down.

 

When choosing between synchronous and asynchronous replication, you have to take into consideration <b>replication lag</b>. If you are reading from a follower, you might get data that is not as up to date as data in the leader or some of the other followers. There are several situations that should be considered when discussing replication lag:

<ul><li><b>Read-after-write consistency</b>: Users should always see the data they submitted themselves.<ul><li>If the user submits a write that goes to the leader and then attempts to read the data they just submitted, the read request might go to a follower node that does not yet have that data. This will make it appear to the user that data is lost.</li><li>It might be that if the user refreshes the page, their data may be there. For some use cases, this is enough. For others, users may not be pleased with their data appearing to be gone.</li><li>Solutions: If this data is on a user’s personal profile, a simple solution would be to have all personal profile reads served from the leader. Another option is to track the last user update time and if there has been an update, to make all user reads from leader for one minute.</li></ul></li><li><b>Monotonic reads</b>: After a user has seen the data at one point in time, they shouldn’t later see the data from some earlier point in time.<ul><li>This could occur when the user makes two subsequent read requests: the first sees updated data and the second sees data that has not yet been fully updated from the leader. This will give the appearance of moving back in time.</li><li>To solve this, you can require that each client pick a follower and always read from that follower (if that follower fails, the client will be reassigned). This does not mean that the client will always see the most up-to-date date, but it does ensure that the data moves in a linear time direction.</li></ul></li><li><b>Consistent prefix reads</b>: Users should see the data in a state that makes causal sense: for example, only seeing a reply after first seeing the question.</li></ul>

 

 

Adding a new follower:

<ul><li>Snapshot the leaders data</li><li>Copy the snapshot to the new replica</li><li>Connect the follower to the leader and request changes since the snapshot occurred</li><li>Process the backlog of changes and come online.</li></ul>

 
 
  

<b>Handling Node Outages</b>:

Nodes can go down unexpectedly or for planned maintenance.

<ul><li>Follower failure: Followers that go down and come back online can recover easily via <b>catch-up recovery</b>. When the node recovers from crash, it will request all changes that it missed and apply those to its local data.</li><li>Leader failure: Leader failure is a bit trickier and is addressed via <b>failover</b>. When the leader fails, a follower needs to be promoted to be the new leader. Then, all of the clients need to be reconfigured to point to the new leader and all of the followers need to consume changes from the new leader.</li></ul>

 

Detailed failover steps:

<ol><li><b>Determine the leader has failed</b>: No foolproof way to detect a failure so often just use a simple timeout.</li><li><b>Choose a new leader</b>: A new leader can be chosen through an election (chosen by majority of remaining nodes) or appointed by a previously elected controller node. The best candidate is usually the node with the most up to date data changes from the old leader as this will minimize data loss. Getting all nodes to agree on a new leader is a consensus problem.</li><li><b>Reconfigure the system to use the new leader</b>: Clients now send write requests to the new leader. If the old leader comes back online, it might believe that it is still the leader. The system needs to ensure that the old leader becomes a follower and recognizes the new leader.</li></ol>

 

There are many things that can go wrong with failover:

<ul><li>If asynchronous replication is used, and the old leader had some unreplicated writes when it went down, what should happen to those writes? The most common solution is that they should be discarded (which reduces durability of the system’s data).</li><li>In certain fault scenarios, it is possible for two nodes to believe that they are the leader (a leader appears down because it has exceeded its timeout. A new leader is selected. The old leader recovers and comes back online). This is a situation known as <b>split brain</b>. If both nodes accept writes and there is no process for resolving conflicts, then this can lead to lost or corrupted data. As a safety catch, some systems have a means to shutdown a node if two are detected as being leaders.</li><li>There are tradeoffs when seeing a leader timeout. Too long of a timeout causes unnecessary downtime: nodes may be waiting on a leader that will not recover. Too short of a timeout causes too many failovers: leader nodes may routinely experience high load or slow networks and will violate there timeout when they are still online and healthy.</li></ul>

All of these issues are in fact the fundamental problems of distributed systems: node failures, unreliable networks, and tradeoffs around replication consistency, durability, availability, and latency.

 

<b>Replication Logs</b>:

A common way of doing replication logs is write ahead logs (WAL): The leader creates a write ahead log of all the data changes and sends that log out to its followers to run.

 

 

Multi-leader Replication:

Single-leader replication is a common choice because it is fairly easy to reason about and understand. Multi-leader and leaderless replication can be more robust in the presence of faults, at the cost of being more difficult to reason about and only providing weak consistency guarantees.

Multi-leader replication is where multiple nodes are designated as leaders, and send updates to followers as well as other leaders.

 

A major downside of single leader replication is that if you can’t access the leader for writes, everything goes down – this makes your leader a single point of failure. It rarely makes sense to use multileader within a single data center because the added complexity outweighs the benefits. If you have multiple data centers, having a leader in each can be a good idea.

image:Multi-leader-replication.png~Multi-leader replication

<b>Offline Leaders</b>:

One use case for multileader replication is clients that need to work offline. Each user device has a copy of the data, and when that device goes offline, it becomes its own leader. When it comes back online, it transmits its changes to the other leaders just like a datacenter leader would.

 

 

<b>Handling Write Conflicts</b>:

A major downside of multileader replication is that the same data may be concurrently modified in two different data centers and that those conflicts then must be resolved.

<ul><li><b>Conflict avoidance</b>: The simplest strategy is to avoid write conflicts. If you can ensure that all writes to a certain piece of data go to the same leader, then there is no need for additional conflict resolution.</li><li><b>Converging towards a consistent state</b>: One way to converge towards a consistent state is to take the most recent update to the data: last write wins (LWW). Although popular, LWW is dangerously prone to data loss. Another approach is to keep conflicts and prompt the user to resolve them via application code (the application could display a prompt to the user such as: “there is a conflict in the data, choose which result to keep”). Converging towards a consistent state means that the data will eventually get updated across all replicas. This is known as <b>eventual consistency</b>.</li></ul>

 

You can resolve conflicts on write or on read. On read keeps both pieces of conflicting data until the data is accessed. This would be a situation where you could ask the user to resolve the conflict. On write conflict resolution is handled pragmatically, via solutions such as LWW or merging the conflicting data together.

 

<b>Replication Topography</b>:

Replication topography describes the communication paths along which writes are propagated from one node to another.

image:Replication-topographies.png~Replication topographies

<ul><li><b>All-to-all</b>: The most general replication topography. Every leader sends its writes to every other leader.</li><li><b>Circular</b>: Each node receives writes from another node, and then forwards those along to the next node, including any additional writes that it might have.</li><li><b>Star</b>: A designated root node forwards writes to all other nodes. Each node gives their individual writes to that node and it distributes them.</li></ul>

Circular topographies can suffer from fault tolerance issues as a node failure can interrupt the flow of updates. The more densely connected a topography is (like all-to-all topographies), the more fault tolerant it is as updates can travel down different paths if some nodes have failed.

 

<b>Leaderless Replication</b>:

Leaderless replication abandons the idea of a leader and instead users write to many or all of the replicas. This was one of the earliest methods for distributed databases and one that has recently come back in favor with Amazon’s in-house Dynamo database (and those modeled after it).

image:Read-Write-Quorum.png~Read/Write Quorum


This image shows an example of a leaderless database. The client sends writes to as many of the nodes as it can. One node is offline so it does not receive the update. When a client reads a value, it attempts to read from as many nodes as possible. The previously offline node has come online so we now have one node with stale data and two nodes with current data. The read checks for a quorum – for a certain (often majority) of node having the same value. In this case, the quorum shows our new value. This is what is returns to the client.

When writing, the client will also check for a write quorum – a threshold of successful writes that must be present for the write to be considered successful.

 

The quorum value for reads and writes is configurable. Sometimes it is a simple majority for both reads and writes. One possible configuration is to require that all nodes confirm a write for it to be successful. You then only need to check a single node for reads. This makes reads very fast and allows many nodes to go down before reads are affected. This works well for datasets with low writes and high reads. The tradeoff is that a single failed node prevents writes.

 

When nodes come online, they need to self repair in order to get caught up on what they missed. There are two often used approaches:

<ol><li><b>Read repair</b>: When a client reads for a value, it may find several stale values (as in our above example). When this happens, it updates those stale values to match its quorum. This approach works well for values that are read often. Data that is not frequently read could remain stale for extended periods of time.</li><li><b>Anti-entropy process</b>: A background process that runs and checks for and  subsequently updates stale data.</li></ol>

 

Sloppy quorums ask the question: if enough nodes are down that you can’t form a quorum, should you return an error or accept the write onto the nodes that are still online. By still accepting writes, you increase availability as you can continue to accept writes when a high number of nodes are offline or unavailable. The downside is that you lose the guarantee that your reads will always have up to date values.

 

<b>Dealing with concurrency</b>:

Concurrency does not mean that two events occurred at exactly the same time but rather that they were unaware of each other. In fact, due to issues with clocks in distributed data systems, it can be quite challenging to determine exactly what time two events occurred.


<b>Last Write Wins (LWW)</b>: A popular choice for conflict resolution where you discard all but the most recent update. The downside is that a user who saw an old value and wrote at almost the same time as someone else, might be surprised when the other user’s value appears instead of their own.


line:

 

 




 

 