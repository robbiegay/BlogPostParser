<h3 className="text-center">Chapter 11: Stream Processing</h3>

<br />

<blockquote class= "blockquote">
	<p class="text-muted">
		<small>
			"A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system."
		</ small>
	</ p>
	<footer class="blockquote-footer"> John Gall, <cite title="Systemantics (1975)">Systemantics (1975)</ cite></ footer>
</ blockquote>

<br />

<p>
	<i>The above quote is known as Gall’s Law. Interestingly, Gall did not work in computers but was a Pediatrician and, in addition to his work on systems theory, also write a book on an ancient Egyptian queen.</i>
</p>

<br />

<br />

<p>
	<b>Stream Processing</b>
</p>

<p>
	Batch processing processes data in fixed increments: every minute, once per day, etc. Stream processing processes data continuously. This reflects the way that many data sources produce data: a user produced data yesterday, will produce data today, and will likely produce more data tomorrow. A <b>stream</b> refers to data that is incrementally made available over time.
</p>

<br />

<br />

<p>
	<b>Transmitting Event Streams</b>
</p>

<p>
	In stream processing, a record is known as an <b>event</b>. An event is a small, self-contained, immutable object that contains the details of something that happened and a timestamp (usually a time-of-day clock). An event could record an action a user took on a website or could be a sensor reading from a scientific probe. 
</p>

<br />

<p>
	An event is generated by a <b>producer</b> (<b>publisher, sender</b>) and potentially processed by multiple <b>consumers</b> (<b>subscribers, recipients</b>). In streaming systems, events are usually grouped together by <b>topic</b> or <b>stream</b>.
</p>

<br />

<p>
	One way to notify consumers of a new event is to have them <b>poll</b> (periodically check) an event source such as a database. This is expensive and it is better for consumers to be notified when a new event is added.
</p>

<br />

<br />

<p>
	<b>Messaging Systems</b>
</p>

<p>
	A common approach to notifying consumers of a new event is through a <b>messaging system</b>. 
</p>

<ol><li>What happens if producers send messages faster than consumers can process them. Three options: drop the messages, buffer the messages in a queue, apply <b>back pressure</b> (block the producer from sending more messages).</li><li>What happens if nodes go offline? Are any messages lost? Durability requires some combination of writing to disk and/or replication. If you can afford to lose messages, you can get higher throughput and lower latency on the same hardware.</li></ol>

<p>
	Some methods of notifying consumers are: UDP and making direct HTTP or RPC calls to the consumer (this is how WebHooks work).
</p>

<br />

<br />

<p>
	<b>Message Brokers</b>
</p>

<p>
	An alternative to messaging systems is the use a <b>message broker</b> (<b>message queue</b>) which is a kind of database optimized for handling message streams. This moves the question of durability to the message broker. Although a message broker is a kind of database, it has several difference compared to using a traditional database:
</p>

<ul><li>Databases keep data until it is explicitly deleted. After delivering a message, the broker deletes it.</li><li>Databases have secondary indexes and ways to arbitrarily query data. Brokers only have ways to subscribe and unsubscribe to topics.</li></ul>

<br />

<p>
	When multiple consumers subscribe to the same topic, two patterns are used:
</p>

<ol><li><b>Load balancing</b>: Messages are delivered to <b>one</b> of the consumers.</li><li><b>Fan-out</b>: Each message is delivered to <b>all</b> of the consumers.</li></ol>

<br />

<p>
	In order to delete a message, the broker has to ensure that the consumer has fully processed it. To do this, the broker will only delete a message after it receives an <b>acknowledgment</b>.
</p>

<br />

<p>
	Since messages are deleted after being acknowledged, a new consumer will only process streams that occurred after it was added. There are hybrid approaches that combine the durable storage approach of databases with the low-latency notifications of messaging. This is the idea behind <b>log-based message brokers</b>.
</p>

<br />

<p>
	A log is an append-only file on disk. A consumer receives a message by reading the log file from start to end and then watches the log to wait for new messages to be added. A simple implementation is to use the Unix tool “tail -f” which watches the end of a file for any data being appended. 
</p>

<br />

<p>
	Logs will eventually grow too large so a process is used to delete or move older log to long term storage. The buffer can typically be quite large, allowing the log to hold hours or days of messages depending on message throughput. The bounded-size buffer is known as a <b>circular buffer</b> or <b>ring buffer</b>. If a consumer falls too far behind, it could fall behind this cutoff point. It can therefore be useful to set up alerts if a consumer fall too far behind.
</p>

<br />

<p>
	Logs allow for both message durability and for messages to be replayed to get the same results – a benefit that batch processing already provided.
</p>

<br />

<br />

<p>
	<b>Change Data Capture</b>
</p>

<p>
	Most database replication logs are not public APIs. There is growing interest in <b>change data capture (CDC)</b> that allows you to track changes to a database. This allows you to use those changes on derived data systems to keep them up to date when data in your system of record database changes.
</p>

<br />

<br />

<p>
	<b>Event Sourcing</b>
</p>

<p>
	<b>Event sourcing</b> is a technique developed in the <b>domain-driven design (DDD)</b> community. Similar to change data capture, event sourcing involves storing all changes to the application state in a log of change events. Events are designed to capture high-level application changes. 
</p>

<br />

<p>
	Events are immutable and cannot be deleted. If a user action involves deleting something like a course enrollment, that does not mean that the enrollment never happened but rather that it happened and was later deleted. In order to keep an accurate history of application events, both the create and delete event are retained.
</p>

<br />

<p>
	Event sourcing can be useful because it decouples events from logic that processes them. If we have a log of all the events that occurred at the application level, at means that we can later make changes to how we want to handle events and can retroactively process old events. For example, if an airline wants to add a rewards program, it can process old flights because it has a record of all application level activity and only has to change the logic that processes these events. Events are like a history of facts, and as long as we have a comprehensive and accurate history, we can make sense of those facts at a later date.
</p>

<br />

<br />

<p>
	<b>Commands and Events</b>
</p>

<p>
	An initial request is known as a <b>command</b>. A command can fail and it is, therefore, only after it is successfully processed that it becomes an <b>event</b>. At the point when an event is generated, it becomes a <b>fact</b> and cannot later be deleted. A consumer of an event stream cannot reject an event. If something needs to be rejected, it needs to happen at the command level.
</p>

<br />

<br />

<p>
	<b>State, Streams, and Immutability</b>
</p>

<p>
	No matter how the state changes, there are always a sequence of events that caused those changes. Even as things are done and undone, the fact remains that those changes occurred. The changelog represents the evolution of state over time. The application state is what you get when you integrate an event stream over time.
</p>

<br />

<p>
	A database can be viewed as a cache of the application state at the current time – a cache of the latest values from the changelog.
</p>

<br />

<p>
	Immutable events have several advantages. If you deploy buggy code and it makes database changes, it becomes hard to revert those changes. If events are immutable, any change events can be reverted by adding the inverse of those events to the eventing system (e.g. a create event is undone by a delete event). Additionally, there are use cases where you may want to know that an event that would otherwise be undone was taken: a customer adds and then removes an item from their shopping cart. Having a record of this series of events can be useful for analytical purposes (perhaps to offer ads or discounts to that consumer to persuade them to come back and purchase that item). 
</p>

<br />

<p>
	There are times when you may need to actually delete events from an immutable collection:
</p>

<ul><li>Datasets with high churn may grow too large over time and a compaction or garbage collection processes may be needed.</li><li>Legal reasons such as EU privacy laws that require user data to be deleted upon request. In this case, it is not enough to append a “delete” entry onto the log, the history must be rewritten as if certain events belonging to that user never occurred.</li></ul>

<br />

<p>
	Deletes are surprisingly hard to implement. Filesystems, storage engines, and SSDs often write to a new location rather than overwriting in place – this means that deleted data may still be recoverable for a time. Backups are deliberately immutable to prevent corruption and user data could continue to live on in backups. Deletion often comes down to making it harder to retrieve the data rather than ensuring that no trace of it remains.
</p>

<br />

<br />

<p>
	<b>Processing Streams</b>
</p>

<p>
	There are three things that you can do with an event stream:
</p>

<ol><li><b>Send it to a database</b>: This is a good way to keep a database up to date.</li><li><b>Send it to a human</b>: update a dashboard, send an email, etc.</li><li><b>Send it to another event stream</b>: Streams form a pipeline creating derived datasets.</li></ol>

<br />

<p>
	A piece of code that processes an event stream is knowns as a <b>job</b> or an <b>operator</b>. A stream processor consumes an input of streams as read-only and writes an output to a different location in an append-only fashion. 
</p>

<br />

<p>
	This all sounds very similar to batch processing but there are some differences: 
</p>

<ul><li>Streams operate on unbounded inputs.</li><li>Without a bounded input, the sort function common in batch processing does not make much sense.</li><li>Batch jobs that run for minutes or hours can be easily restarted. A stream job that has been running for years likely cannot be restarted.</li></ul>

<br />

<br />

<p>
	<b>Uses of Stream Processing</b>
</p>

<p>
	<b>Complex Event Processing (CEP)</b>: These type of systems use a high-level declarative query language like SQL or a GUI to describe patterns that events should match. Databases store data and allow transient queries to be run against them. CEP engines work in reverse: they store queries and when events come in, they are matched against stored queries . When a match is found, a <b>complex event</b> is emitted.
</p>

<br />

<p>
	<b>Stream Analytics</b>: Stream analytics are similar to CEP but more focused on aggregations and statistical metrics over time. The time interval over which you aggregate is known as a <b>window</b>.
</p>

<br />

<br />

<p>
	<b>Reasoning About Time</b>
</p>

<p>
	Stream processors often have to deal with time. Clock time is often used as it is simpler to deal with. Event time (time at which the event occurred) and processing time (time at which it was processed by the stream processor) are different and should not be mixed. 
</p>

<br />

<p>
	Sometimes an event processor will processes windows of time. If it is processing the 37th minute of the hour, when can it be sure that all events for that minute have arrived and been processed? What if a <b>straggler</b> event comes in several minutes or hours later?
</p>

<ol><li><b>Ignore the straggler</b>: They are probably small in number. You can track the number that have been ignored as a metric and fire an alert if the number starts spiking.</li><li><b>Publish a correction</b>: A correction is an updated window with the straggler added.</li></ol>

<br />

<p>
	By tracking the times at which: an event occurred (device clock), event was sent to the server (device clock) and event was received (server clock), you can estimate the offset between device and server clock.
</p>

<br />

<p>
	<b>Time Windows</b>
</p>

<p>
	There are several types of time windows:
</p>

<ul><li><b>Tumbling window</b>: A tumbling window with a fixed length with each event belong to exactly one window. For example, a 1-minute tumbling window would store events as follows: 10:<b>03:00</b>-10:<b>03:59</b>, and 10:<b>04:00</b>-10:<b>04:59</b>.</li><li><b>Hopping window</b>: A hopping window also has a fixed length but overlaps by a <b>hop</b> amount to provide smoothing of events. For example, a 5-minute tumbling window with a hop of 1-minute would store events as follows: 10:<b>03</b>:00-10:<b>07</b>:59, 10:<b>04</b>:00-10:<b>08</b>:59. Notice how each new window only <b>hops</b> forward by <b>1 minute</b>.</li><li><b>Sliding window</b>: A sliding window includes all events that happened within a certain interval of each other. For example, a 5-minute sliding window would include all events within 5 minutes of each other, which would place 10:<b>03:39</b> and 10:<b>08:12</b> in the same window.</li><li><b>Session window</b>: A session window has no time limit and instead groups all events by the same user that occur before the user becomes inactive (timeout). Sessionization is a common requirement for website analytics.</li></ul>

<br />

<br />

<p>
	<b>Stream Joins</b>
</p>

<ul><li><b>Stream-stream join (window join)</b>: For example, you may want to compare searches and clicks to get a click-through rate. The search stream and click stream will need to maintain state for an interval of time (say, 1 hour), and any matches between searches and clicks are emitted as a click-through event.</li><li><b>Stream-table join (stream enrichment)</b>: A stream can be enriched by adding user from a database. For example: an event with a user could be enriched with information from a user database.</li><li><b>Table-table join</b></li></ul>

<br />

<br />

<p>
	<b>Fault Tolerance</b>
</p>

<ul><li><b>Microbatching</b>: Break the stream into small blocks and treat each block like a mini-batch. Microbatches are typically around 1-second in length.</li><li><b>Checkpointing</b>: Generate periodic, rolling checkpoints of state and write them to durable storage.</li><li><b>Idempotence</b>: Make it so that even if you rerun some action, the results stays the same. This can either be because an action is naturally idempotent, or because it was made to be so via something like monotonically increasing id’s that will only be applied if it is greater than the current id in the target system.</li></ul>

<br />

<br />

<br />

<br />

<p>
	<b>Chapter Summary</b>:
</p>

<p>
	Streaming is similar to batch processing in many ways except that is done continuously and on an unbounded (never ending) dataset. While batch processing uses a distributed filesystem, streaming uses message brokers and event logs.
</p>

<p>
	Two types of message brokers:
</p>

<ol><li><b>AMQP/JMS-style message broker</b>: The broker assigns messages to individual consumers, and consumers acknowledge the messages when they have been successfully processed. Messages are deleted from the broker once the have been acknowledged. This is useful when exact ordering of messages is not important and when there is no need to go back and read old messages.</li><li><b>Log-based message broker</b>: The broker assigns all messages in a partition to the same consumer node and always delivers messages in the same order. The broker retains messages on disk and it is possible to read old messages.</li></ol>

<br />

<p>
	Where streams come from: user activity events, sensors providing periodic readings, data feeds (e.g., market data in finance). Writes to a database can also be thought of as a stream: we can capture the changelog (history of changes to the database) either explicitly through event sourcing or implicitly through change data capture.
</p>

<br />

<p>
	Time in a stream processor can be difficult to deal with, particularly dealing with straggler events that arrive after you thought a window was complete.
</p>

<br />

<p>
	Three types of joins in a stream processor:
</p>

<ol><li><b>Stream-stream</b>: A join operator searches for events that match within two streams in a window of time (or the same stream in a self-join).</li><li><b>Stream-table</b>: One input is a stream of events and the other is a changelog of a database. The changelog keeps the local database up to date.</li><li><b>Table-table</b>: Both input streams are database changelogs, and a change on one side is updated to the other. The result is a stream of changes to the materialized view of the join between the two tables.</li></ol>

<br />

<p>
	Fault tolerance and exactly-once semantics must be achieved in a stream processor.
</p>

<br />

<p>
	As with batch processing, we need to discard any partial or failed tasks, but because the streams run continuously, we cannot discard the entire results. Instead, a finer-grained set of tools must be used: micro batching, checkpointing, transactions, or idempotent writes.
</p>

<br />

<br />


<br />
<hr />
<br />


<br />

<br />

<br />

<br />

<h3 className="text-center">Chapter 12</h3>

<br />

<p>
	In chapter 12, Martin Kleppmann switches from discussing the current state of distributed systems to proposing ideas and approaches that he believes will improve how we build and design applications.
</p>

<br />

<p>
	This section is more speculative and I have decided not to go too in-depth with the final chapter summary as this information is likely still rapidly changing. Instead, I recommend that you read chapter 12 if you want to know about exciting new developments in distributed systems!
</p>

<br />

<br />

<p>
	<b>Data Integration</b>
</p>

<p>
	The data you use in your application can have many uses: “The range of different things you might want to do with data is dizzyingly wide”. It is unlikely that you will find a single tool or technology that is well suited for all of these uses.
</p>

<br />

<p>
	If it is possible for you to funnel all user data through a single system that decides on the ordering of writes, it then becomes simpler for your derived systems to process writes in a consistent order. Deciding on the order of events is known as <b>total order broadcast</b> and can be challenging to do right. The most straight forward way is to use a single leader node, but this brings issues with availability (the leader node is a single point of failure) and with geographically distributed data sources having to route traffic through a node that is potentially located across the globe. The current application patterns for addressing this problem are not the best but over time there may be advances in was to capture casual dependencies effectively and allow derived state to be maintained accurately without going through the bottleneck of total order broadcast.
</p>

<br />

<br />

<p>
	<b>Batch and Stream Processing</b>
</p>

<p>
	The goal of data integration is for the right data to end up in the right form in the right places. To achieve this, you need to build data systems that: consumes inputs; performs transforms, joins, filtering, aggregating, trains models, and evaluates data; and eventually writes it to various outputs. Batch and stream processors are tools for this job.
</p>

<br />

<p>
	Batch processing has a strong functional flavor: deterministic, pure functions whose output depends only on its input and that has no side effects. Inputs are treated as immutable and outputs are written in an append only manner.
</p>

<br />

<p>
	Batch processing allows large amounts of data to be reprocessed to derive new views from existing data. Stream processing allows new data to be processed with minimal delay.
</p>

<br />

<p>
	One benefit of being able to reprocesses data via batch processing is schema migration. Derived views allow you to do a gradual schema migration. You can create two views: one matches your old schema, and one matches your new schema. You can then slowly switch clients over to the new schema. If something wrong, you can easily switch a client back to the old schema. Once all clients have been switched over, you can depreciate the old view that matches your old schema.
</p>

<br />

<br />

<p>
	<b>The Lambda Architecture</b>
</p>

<p>
	If batch processing is used to process historical data and stream processing is used to process recent updates, how do you combine the two? Lambda architecture is a proposal that is gaining a lot of attention. 
</p>

<br />

<p>
	<b>Lambda architecture</b> works by: incoming data is recorded into an immutable, append only dataset. Both a stream and batch processor run in parallel to process data from this immutable dataset. Stream processing is faster but seen as more error prone. The stream processor provides results quickly but with a chance of errors. A batch processor later processes that same data and provides a more updated result. This gives you quick results that are mostly correct and later a form of “eventual correctness” from batch processing.
</p>

<br />

<p>
	Some issues with the Lambda architecture include:
</p>

<ul><li>Having to maintain duplicated logic in both the batch and stream processor.</li><li>Having to merge results from the batch and stream processor to respond to user requests.</li><li>Running a batch process frequently can be expensive on large datasets.</li></ul>

<br />

<br />

<p>
	<b>Unbundling Databases</b>
</p>

<p>
	Databases and operating systems can be viewed as doing similar things: they both store data and allow a user to access that data. A database stores data in a data model while an operating system stores data in a file system. Both are <b>information management systems</b>.
</p>

<br />

<p>
	There are some practical differences: file systems do not respond well to a directory containing 10 million small files while databases do that quite efficiently. Unix tools present programmers with logical but fairly low level tools over a hardware abstraction. Databases provide a high level query language that don’t require the user to know the implementation details that occur as the query is optimized.
</p>

<br />

<br />

<p>
	<b>Composing Data Storage Technologies</b>
</p>

<p>
	There are several features provided by a database:
</p>

<ul><li><b>Secondary Indexes</b>: Efficient search for records based on the value of a field.</li><li><b>Materialized Views</b>: A precomputed cache of query results.</li><li><b>Replication Logs</b>: Keeps copies of the data on other nodes up to date.</li><li><b>Full-test search indexes</b>: Allow keyword search in text.</li></ul>

<br />

<p>
	Full text searches, materialized views, and replicating database changes are all tasks that we have tried to do with batch and streaming jobs.
</p>

<br />

<p>
	Database do a lot of things that we would like to do with distributed systems. These database functionalities are not public APIs so we cannot take advantage of these functionality easily. Instead, it is possible to build data systems that operate like a database. 
</p>

<br />

<ul><li><b>Federated databases (unifying reads)</b>: It is possible to provide a unified query interface to a wide variety of underlying storage engines and processing methods, an approach known as a <b>federated database</b> or <b>polystore</b>. This allows you to use a high level query language to query across a variety of systems, hiding complicated system implementation details.</li><li><b>Unbundled databases (unifying writes)</b>: Writes on a unified system are harder to implement. </li></ul>

<br />

<p>
	This processes of unbundling database systems and composing specialized storage and processing systems is known as the <b>database inside-out</b> approach. Composing your own database-like system out of various tools can be advantageous but should only be undertaken if current tools do not already meet your needs.
</p>

<br />

<br />

<br />

<p>
	<b>Designing Applications Around Dataflow</b>
</p>

<p>
	Web applications often separate stateless servers from state stored in a database. The values in the database are treated like a mutable variable but without implementing the observer pattern, there is no built-in way to know when your variables have changes other then periodically checking. Rather, we can have our stateless application subscribe to a stream of changes. Our application will receive a notification when data they rely on changes.
</p>

<br />

<p>
	<b>Miscellaneous Note</b>
</p>

<p>
	Test your backups BEFORE you need them. If you create backups regularly, but only attempt to restore from them in an emergency, you could be in for an unpleasant surprise.
</p>

<br />

<br />

<p>
	<b>Chapter Summary</b>
</p>

<p>
	There is no one tool to solve distributed system problems and instead multiple tools should be composed together. One way to solve this data integration problem is by using batch processing and event streams to allow data changes to flow between different systems. Certain systems are designated as systems of record and other data is derived through transformations. By making these derived datasets and transformations asynchronous and loosely coupled, we can prevent problems in one part of the system from spreading to another part. You can derive new data or fix corrupted data by rerunning transformations on your system of record.
</p>

<br />

<p>
	These processes are quite similar to what databases already do internally and we can recast the idea of dataflow applications as an unbundling of the components of a database, and building an application by composing these loosely coupled components.
</p>

<br />

<p>
	<b>Ethics</b>
</p>

<p>
	The book closes with a section on data ethics: Data applications can seriously affect people’s lives, making decisions that are difficult to appeal (<b>algorithmic prison</b>), leading to discrimination and exploitation, the normalization of surveillance, and exposing intimate information. With the risk of data breaches and changing governing bodies (a government that supports data privacy might change to one that does not; a company can change ownership), even once well intentioned data can be used in unethical ways.
</p>

<br />

<p>
	As a thought experiment, Kleppmann suggests replacing the word “data” with “surveillance” when talking about big <s>data</s> surveillance companies. As such, you might hear a company say: “<i>We collect real-time surveillance and store it in our surveillance warehouse.</i>” Even this book might be renamed: <i>Designing Surveillance Intensive Applications.</i>
</p>

<br />

<p>
	Some critics have pointed out that user data should be handled carefully, as if it were a <b>hazardous</b> or <b>toxic asset</b>. Since data is the result of interaction with many applications, it is sometimes called <b>data exhaust</b>.
</p>

<br />

<p>
	Even though companies may currently protect user data, change of ownership could result in data being sold later. Likewise, a company that never intends to share user data may be forced to if a police state comes to power. Even the most extreme governments used to only be able to dream about putting a microphone and camera in every home and now we do it willingly. We are lucky that these spying devices have not already been put to bad use.
</p>

<br />

<br />

<p>
	<b>Closing Remarks</b>
</p>

<p>
	Kleppmann closes by saying this:
</p>

<p>
	“<i>As software and data are having such a large impact on the world, we engineers must remember that we carry a responsibility to work toward the kind of world that we want to live in: a world that treats people with humanity and respect. I hope that we can work together toward that goal.</i>”
</p>

<br />

<br />

<br />

<p>
	<b>Code of Ethics Resource</b>
</p>

<p>
	ACM Code of Ethics and Professional Conduct: <a href="https://www.acm.org/code-of-ethics">Code of Ethics</a> 
</p>

<br />

<br />

<br />

<br />

<br />

<br />

